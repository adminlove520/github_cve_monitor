#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
GitHub CVE Monitor - æ¯æ—¥æ•°æ®ç”Ÿæˆè„šæœ¬
ä»README.mdä¸­æå–CVEæ•°æ®ï¼ŒæŒ‰æ—¥æœŸåˆ†ç»„ç”Ÿæˆæ¯æ—¥JSONæ–‡ä»¶
"""

import os
import json
import re
from datetime import datetime, timedelta
from collections import defaultdict
import argparse

def parse_readme(readme_path):
    """è§£æREADME.mdæ–‡ä»¶ï¼Œæå–CVEæ•°æ®"""
    try:
        with open(readme_path, 'r', encoding='utf-8') as f:
            content = f.read()
    except FileNotFoundError:
        print(f"âŒ READMEæ–‡ä»¶æœªæ‰¾åˆ°: {readme_path}")
        return []
    
    # æŸ¥æ‰¾è¡¨æ ¼æ•°æ®å¼€å§‹ä½ç½®
    table_start = content.find('| CVE | ç›¸å…³ä»“åº“')
    if table_start == -1:
        print("âŒ æœªæ‰¾åˆ°CVEè¡¨æ ¼æ•°æ®")
        return []
    
    # æå–è¡¨æ ¼éƒ¨åˆ†
    table_section = content[table_start:]
    lines = table_section.split('\n')[2:]  # è·³è¿‡è¡¨å¤´
    
    cve_data = []
    print(f"ğŸ“‹ å¼€å§‹è§£æCVEæ•°æ®...")
    
    for line_num, line in enumerate(lines, 1):
        line = line.strip()
        if not line or not line.startswith('|'):
            continue
            
        # åˆ†å‰²è¡¨æ ¼åˆ—
        cols = [col.strip() for col in line.split('|') if col.strip()]
        if len(cols) < 4:
            continue
            
        try:
            # æå–CVE ID
            cve_match = re.search(r'CVE[_-]?\d{4}[_-]?\d+', cols[0])
            if not cve_match:
                continue
            cve_id = cve_match.group()
            
            # æå–ä»“åº“ä¿¡æ¯
            repo_info = cols[1]
            
            # æå–æè¿°
            description = cols[2]
            
            # æå–æ—¥æœŸ - æ”¯æŒå¤šç§æ ¼å¼
            date_str = cols[3].strip()
            date_patterns = [
                r'(\d{4}-\d{2}-\d{2})T\d{2}:\d{2}:\d{2}Z',  # ISOæ ¼å¼
                r'(\d{4}-\d{2}-\d{2})',  # ç®€å•æ—¥æœŸæ ¼å¼
            ]
            
            parsed_date = None
            for pattern in date_patterns:
                date_match = re.search(pattern, date_str)
                if date_match:
                    try:
                        parsed_date = datetime.strptime(date_match.group(1), '%Y-%m-%d').date()
                        break
                    except ValueError:
                        continue
            
            if not parsed_date:
                print(f"âš ï¸  ç¬¬{line_num}è¡Œæ—¥æœŸæ ¼å¼æ— æ³•è§£æ: {date_str}")
                continue
            
            cve_data.append({
                'cve_id': cve_id,
                'repo_info': repo_info,
                'description': description,
                'date': parsed_date.isoformat(),
                'raw_date': date_str
            })
            
            if line_num % 1000 == 0:
                print(f"ğŸ“Š å·²å¤„ç† {line_num} è¡Œï¼Œæå–åˆ° {len(cve_data)} ä¸ªCVE")
                
        except Exception as e:
            print(f"âš ï¸  ç¬¬{line_num}è¡Œè§£æå¤±è´¥: {e}")
            continue
    
    print(f"âœ… è§£æå®Œæˆï¼æ€»è®¡æå–åˆ° {len(cve_data)} ä¸ªCVE")
    return cve_data

def group_by_date(cve_data):
    """æŒ‰æ—¥æœŸåˆ†ç»„CVEæ•°æ®"""
    daily_data = defaultdict(list)
    
    for cve in cve_data:
        date_key = cve['date']
        daily_data[date_key].append(cve)
    
    # è½¬æ¢ä¸ºæ™®é€šå­—å…¸å¹¶æ’åº
    sorted_daily = dict(sorted(daily_data.items()))
    
    print(f"ğŸ“… æ•°æ®åˆ†ç»„å®Œæˆ:")
    print(f"   - æ—¥æœŸèŒƒå›´: {min(sorted_daily.keys())} åˆ° {max(sorted_daily.keys())}")
    print(f"   - æ€»å¤©æ•°: {len(sorted_daily)} å¤©")
    
    return sorted_daily

def generate_json_files(daily_data, output_dir):
    """ç”Ÿæˆæ¯æ—¥JSONæ–‡ä»¶"""
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
        print(f"ğŸ“ åˆ›å»ºè¾“å‡ºç›®å½•: {output_dir}")
    
    generated_files = []
    
    for date_str, cves in daily_data.items():
        # æ–‡ä»¶åæ ¼å¼: YYYY-MM-DD.json
        filename = f"{date_str}.json"
        filepath = os.path.join(output_dir, filename)
        
        # å‡†å¤‡JSONæ•°æ®
        json_data = {
            'date': date_str,
            'count': len(cves),
            'cves': cves,
            'generated_at': datetime.now().isoformat(),
            'metadata': {
                'total_cves': len(cves),
                'date_range': date_str,
                'source': 'README.md'
            }
        }
        
        # å†™å…¥JSONæ–‡ä»¶
        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(json_data, f, ensure_ascii=False, indent=2)
            
            generated_files.append({
                'file': filename,
                'date': date_str,
                'count': len(cves),
                'path': filepath
            })
            
        except Exception as e:
            print(f"âŒ ç”Ÿæˆæ–‡ä»¶å¤±è´¥ {filename}: {e}")
    
    return generated_files

def generate_summary(generated_files, output_dir):
    """ç”Ÿæˆæ±‡æ€»ä¿¡æ¯"""
    summary = {
        'generated_at': datetime.now().isoformat(),
        'total_files': len(generated_files),
        'total_cves': sum(f['count'] for f in generated_files),
        'date_range': {
            'start': min(f['date'] for f in generated_files) if generated_files else None,
            'end': max(f['date'] for f in generated_files) if generated_files else None
        },
        'daily_stats': generated_files,
        'recent_7_days': []
    }
    
    # è®¡ç®—æœ€è¿‘7å¤©æ•°æ®
    if generated_files:
        recent_files = sorted(generated_files, key=lambda x: x['date'])[-7:]
        summary['recent_7_days'] = recent_files
    
    # ä¿å­˜æ±‡æ€»æ–‡ä»¶
    summary_path = os.path.join(output_dir, 'daily_summary.json')
    try:
        with open(summary_path, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        print(f"ğŸ“Š æ±‡æ€»æ–‡ä»¶å·²ç”Ÿæˆ: {summary_path}")
    except Exception as e:
        print(f"âŒ ç”Ÿæˆæ±‡æ€»æ–‡ä»¶å¤±è´¥: {e}")
    
    return summary

def main():
    parser = argparse.ArgumentParser(description='ç”Ÿæˆæ¯æ—¥CVEæ•°æ®JSONæ–‡ä»¶')
    parser.add_argument('--readme', '-r', 
                       default='../docs/README.md',
                       help='README.mdæ–‡ä»¶è·¯å¾„ (é»˜è®¤: ../docs/README.md)')
    parser.add_argument('--output', '-o',
                       default='../docs/Data/daily',
                       help='è¾“å‡ºç›®å½• (é»˜è®¤: ../docs/Data/daily)')
    parser.add_argument('--verbose', '-v', 
                       action='store_true',
                       help='æ˜¾ç¤ºè¯¦ç»†è¾“å‡º')
    
    args = parser.parse_args()
    
    print("ğŸš€ GitHub CVE Monitor - æ¯æ—¥æ•°æ®ç”Ÿæˆå™¨")
    print("=" * 50)
    
    # è§£æREADME
    print(f"ğŸ“– è¯»å–READMEæ–‡ä»¶: {args.readme}")
    cve_data = parse_readme(args.readme)
    
    if not cve_data:
        print("âŒ æœªæå–åˆ°CVEæ•°æ®ï¼Œè„šæœ¬é€€å‡º")
        return 1
    
    # æŒ‰æ—¥æœŸåˆ†ç»„
    print("\nğŸ“… æŒ‰æ—¥æœŸåˆ†ç»„æ•°æ®...")
    daily_data = group_by_date(cve_data)
    
    # ç”ŸæˆJSONæ–‡ä»¶
    print(f"\nğŸ“ ç”ŸæˆJSONæ–‡ä»¶åˆ°: {args.output}")
    generated_files = generate_json_files(daily_data, args.output)
    
    # ç”Ÿæˆæ±‡æ€»
    print("\nğŸ“Š ç”Ÿæˆæ±‡æ€»ä¿¡æ¯...")
    summary = generate_summary(generated_files, args.output)
    
    # è¾“å‡ºç»“æœ
    print("\n" + "=" * 50)
    print("âœ… å¤„ç†å®Œæˆï¼")
    print(f"ğŸ“ ç”Ÿæˆæ–‡ä»¶æ•°: {len(generated_files)}")
    print(f"ğŸ“Š æ€»CVEæ•°é‡: {summary['total_cves']}")
    print(f"ğŸ“… æ—¥æœŸèŒƒå›´: {summary['date_range']['start']} åˆ° {summary['date_range']['end']}")
    
    if args.verbose and generated_files:
        print("\nğŸ“‹ æœ€è¿‘ç”Ÿæˆçš„æ–‡ä»¶:")
        for f in generated_files[-5:]:  # æ˜¾ç¤ºæœ€è¿‘5ä¸ªæ–‡ä»¶
            print(f"   - {f['file']}: {f['count']} ä¸ªCVE")
    
    return 0

if __name__ == '__main__':
    exit(main())