#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
GitHub CVE Monitor - Wikiç»Ÿè®¡æ•°æ®ç”Ÿæˆè„šæœ¬

åŠŸèƒ½:
1. ä»daily_summary.jsonæå–ç»Ÿè®¡æ•°æ®
2. ç”ŸæˆCVEåˆ†ç±»ç»Ÿè®¡
3. ç”ŸæˆPOC/EXPç»Ÿè®¡
4. è®¡ç®—è¶‹åŠ¿æ•°æ®
5. ç”Ÿæˆä¾›Wikiä½¿ç”¨çš„ç»Ÿè®¡æ•°æ®æ–‡ä»¶
"""

import os
import json
import re
from datetime import datetime, timedelta
from collections import defaultdict, Counter
import argparse
from pathlib import Path

def load_daily_summary(summary_path):
    """åŠ è½½æ¯æ—¥æ±‡æ€»æ•°æ®"""
    try:
        with open(summary_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    except FileNotFoundError:
        print(f"âŒ æ±‡æ€»æ–‡ä»¶æœªæ‰¾åˆ°: {summary_path}")
        return None
    except json.JSONDecodeError:
        print(f"âŒ æ±‡æ€»æ–‡ä»¶æ ¼å¼é”™è¯¯: {summary_path}")
        return None

def load_daily_files(daily_dir, days=30):
    """åŠ è½½æœ€è¿‘Nå¤©çš„æ¯æ—¥JSONæ–‡ä»¶"""
    daily_files = []
    today = datetime.now().date()
    
    for i in range(days):
        target_date = today - timedelta(days=i)
        date_str = target_date.isoformat()
        file_path = os.path.join(daily_dir, f"{date_str}.json")
        
        if os.path.exists(file_path):
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    daily_files.append(data)
            except Exception as e:
                print(f"âš ï¸  æ— æ³•è¯»å–æ–‡ä»¶ {file_path}: {e}")
    
    return sorted(daily_files, key=lambda x: x['date'])

def analyze_cve_types(cve_data):
    """åˆ†æCVEç±»å‹åˆ†å¸ƒ"""
    type_patterns = {
        'è¿œç¨‹ä»£ç æ‰§è¡Œ': [r'RCE', r'remote code execution', r'è¿œç¨‹ä»£ç æ‰§è¡Œ', r'code execution'],
        'æ³¨å…¥æ”»å‡»': [r'injection', r'æ³¨å…¥', r'SQL', r'XSS', r'CSRF', r'OGNL', r'å‘½ä»¤æ³¨å…¥', r'æŒ‡ä»¤æ³¨å…¥'],
        'ææƒæ¼æ´': [r'privilege escalation', r'ææƒ', r'æƒé™æå‡', r'elevation'],
        'ä¿¡æ¯æ³„éœ²': [r'info disclosure', r'information disclosure', r'ä¿¡æ¯æ³„éœ²', r'æ•æ„Ÿä¿¡æ¯'],
        'è·¯å¾„éå†': [r'path traversal', r'traversal', r'directory traversal', r'ç›®å½•éå†'],
        'æ‹’ç»æœåŠ¡': [r'DoS', r'denial of service', r'æ‹’ç»æœåŠ¡'],
        'è®¤è¯ç»•è¿‡': [r'bypass', r'authentication bypass', r'ç»•è¿‡', r'auth bypass']
    }
    
    type_count = defaultdict(int)
    unclassified = 0
    
    for day_data in cve_data:
        for cve in day_data.get('cves', []):
            description = cve.get('description', '').lower()
            classified = False
            
            for cve_type, patterns in type_patterns.items():
                for pattern in patterns:
                    if pattern.lower() in description:
                        type_count[cve_type] += 1
                        classified = True
                        break
                if classified:
                    break
            
            if not classified:
                unclassified += 1
    
    # æ·»åŠ æœªåˆ†ç±»
    if unclassified > 0:
        type_count['å…¶ä»–'] = unclassified
    
    # è½¬æ¢ä¸ºæ’åºåçš„åˆ—è¡¨
    return sorted(type_count.items(), key=lambda x: x[1], reverse=True)

def analyze_poc_exp(cve_data):
    """åˆ†æPOC/EXPç»Ÿè®¡"""
    poc_keywords = ['poc', 'proof of concept', 'éªŒè¯è„šæœ¬', 'æ¦‚å¿µéªŒè¯']
    exp_keywords = ['exp', 'exploit', 'æ¼æ´åˆ©ç”¨', 'åˆ©ç”¨ä»£ç ']
    
    poc_count = 0
    exp_count = 0
    both_count = 0
    neither_count = 0
    
    for day_data in cve_data:
        for cve in day_data.get('cves', []):
            repo_info = cve.get('repo_info', '').lower()
            description = cve.get('description', '').lower()
            content = f"{repo_info} {description}"
            
            has_poc = any(keyword in content for keyword in poc_keywords)
            has_exp = any(keyword in content for keyword in exp_keywords)
            
            if has_poc and has_exp:
                both_count += 1
            elif has_poc:
                poc_count += 1
            elif has_exp:
                exp_count += 1
            else:
                neither_count += 1
    
    return {
        'ä»…POC': poc_count,
        'ä»…EXP': exp_count,
        'POC+EXP': both_count,
        'æ— POC/EXP': neither_count
    }

def calculate_trends(growth_stats, days=7):
    """è®¡ç®—è¶‹åŠ¿æ•°æ®"""
    if len(growth_stats) < days:
        return growth_stats
    
    return growth_stats[-days:]

def generate_vendor_stats(cve_data):
    """ç”Ÿæˆå‚å•†ç»Ÿè®¡"""
    vendor_patterns = {
        'Microsoft': [r'microsoft', r'windows', r'ms-', r'azure', r'office'],
        'Google': [r'google', r'android', r'chrome', r'gcp', r'firebase'],
        'Apple': [r'apple', r'ios', r'macos', r'iphone', r'ipad'],
        'Linux': [r'linux', r'kernel', r'ubuntu', r'debian', r'redhat'],
        'Adobe': [r'adobe', r'acrobat', r'reader', r'photoshop', r'premiere'],
        'Oracle': [r'oracle', r'java', r'mysql', r'plsql', r'database'],
        'Cisco': [r'cisco', r'router', r'switch', r'asa', r'ios'],
        'Apache': [r'apache', r'tomcat', r'httpd', r'struts', r'spark'],
        'Nginx': [r'nginx', r'engine x'],
        'AWS': [r'aws', r'amazon', r'lambda', r'ec2', r's3']
    }
    
    vendor_count = defaultdict(int)
    
    for day_data in cve_data:
        for cve in day_data.get('cves', []):
            description = cve.get('description', '').lower()
            cve_id = cve.get('cve_id', '').lower()
            content = f"{description} {cve_id}"
            
            for vendor, patterns in vendor_patterns.items():
                for pattern in patterns:
                    if pattern.lower() in content:
                        vendor_count[vendor] += 1
                        break
    
    # è½¬æ¢ä¸ºæ’åºåçš„åˆ—è¡¨
    return sorted(vendor_count.items(), key=lambda x: x[1], reverse=True)

def generate_stats_file(summary, daily_files, output_path):
    """ç”Ÿæˆç»Ÿè®¡æ•°æ®æ–‡ä»¶"""
    # è·å–ç»Ÿè®¡ä¿¡æ¯
    cve_types = analyze_cve_types(daily_files)
    poc_exp_stats = analyze_poc_exp(daily_files)
    vendor_stats = generate_vendor_stats(daily_files)
    trends = calculate_trends(summary.get('growth_analysis', []))
    
    # å‡†å¤‡ç»Ÿè®¡æ•°æ®
    stats = {
        'generated_at': datetime.now().isoformat(),
        'version': '1.0',
        'summary': {
            'total_cves': summary.get('total_cves', 0),
            'date_range': summary.get('date_range', {}),
            'avg_daily_cves': summary.get('statistics', {}).get('avg_daily_cves', 0),
            'active_days': summary.get('statistics', {}).get('active_days', 0),
            'max_daily_cves': summary.get('statistics', {}).get('max_daily_cves', 0)
        },
        'cve_types': dict(cve_types),
        'poc_exp_stats': poc_exp_stats,
        'vendor_stats': dict(vendor_stats[:10]),  # å–å‰10ä¸ªå‚å•†
        'trends': trends,
        'recent_data': daily_files[-7:]  # æœ€è¿‘7å¤©æ•°æ®
    }
    
    # ä¿å­˜ç»Ÿè®¡æ–‡ä»¶
    try:
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        output_dir = os.path.dirname(output_path)
        if output_dir and not os.path.exists(output_dir):
            print(f"ğŸ“ åˆ›å»ºè¾“å‡ºç›®å½•: {output_dir}")
            os.makedirs(output_dir, exist_ok=True)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(stats, f, ensure_ascii=False, indent=2)
        print(f"âœ… ç»Ÿè®¡æ•°æ®æ–‡ä»¶å·²ç”Ÿæˆ: {output_path}")
        return stats
    except Exception as e:
        print(f"âŒ ç”Ÿæˆç»Ÿè®¡æ–‡ä»¶å¤±è´¥: {e}")
        return None

def generate_wiki_md(stats, output_md_path):
    """ç”ŸæˆWikiç»Ÿè®¡æ•°æ®Markdown"""
    if not stats:
        return False
    
    # æå–æ•°æ®
    summary = stats.get('summary', {})
    cve_types = stats.get('cve_types', {})
    poc_exp_stats = stats.get('poc_exp_stats', {})
    vendor_stats = stats.get('vendor_stats', {})
    trends = stats.get('trends', [])
    
    # ç¡®ä¿trendsä¸ä¸ºç©ºæ—¶å†å–æœ€å¤§å€¼
    most_active_day = 'æš‚æ— '
    most_active_count = 0
    if trends:
        most_active = max(trends, key=lambda x: x['daily_count'])
        most_active_day = most_active['date']
        most_active_count = most_active['daily_count']
    
    # ç”ŸæˆMarkdownå†…å®¹
    md_content = f"""
# ç»Ÿè®¡æ•°æ®

æœ¬é¡µé¢å±•ç¤ºGitHub CVEç›‘æ§ç³»ç»Ÿçš„ç»Ÿè®¡æ•°æ®å’Œåˆ†æä¿¡æ¯ï¼Œæ•°æ®è‡ªåŠ¨ä»ç³»ç»Ÿä¸­è·å–å¹¶æ›´æ–°ã€‚

## ğŸ“Š æ€»ä½“ç»Ÿè®¡
- **æ€»CVEè®°å½•æ•°**: {summary.get('total_cves', 0):,}
- **å¹³å‡æ¯æ—¥æ–°å¢**: {summary.get('avg_daily_cves', 0):.1f} ä¸ª
- **æœ€æ´»è·ƒæ—¥æœŸ**: {most_active_day} (å½“æ—¥æ–°å¢ {most_active_count} ä¸ª)
- **ç›‘æµ‹å‘¨æœŸ**: {summary.get('date_range', {}).get('start', 'æš‚æ— ')} è‡³ {summary.get('date_range', {}).get('end', 'æš‚æ— ')}
- **æ´»è·ƒå¤©æ•°**: {summary.get('active_days', 0)} å¤©
- **æ•°æ®æ›´æ–°æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## ğŸ“ˆ æ¯æ—¥å¢é•¿è¶‹åŠ¿

| æ—¥æœŸ | æ¯æ—¥æ–°å¢ | ç´¯è®¡æ€»æ•° | å¢é•¿ç‡ |
|:---|:---|:---|:---|
"""
    
    # æ·»åŠ è¶‹åŠ¿è¡¨æ ¼
    for trend in reversed(trends):  # ä»æ–°åˆ°æ—§æ˜¾ç¤º
        growth_icon = "ğŸ“ˆ" if trend['growth_rate'] > 0 else "ğŸ“‰" if trend['growth_rate'] < 0 else "â¡ï¸"
        md_content += f"| {trend['date']} | {trend['daily_count']} | {trend['cumulative_total']:,} | {trend['growth_rate']:+.1f}% {growth_icon} |\n"
    
    # æ·»åŠ CVEç±»å‹ç»Ÿè®¡
    if cve_types:
        md_content += "\n## ğŸ” CVEåˆ†ç±»ç»Ÿè®¡\n\n| ç±»å‹ | æ•°é‡ | å æ¯” |\n|:---|:---|:---|\n"
        total_types = sum(cve_types.values())
        for cve_type, count in cve_types.items():
            percentage = (count / total_types * 100) if total_types > 0 else 0
            md_content += f"| {cve_type} | {count:,} | {percentage:.1f}% |\n"
    else:
        md_content += "\n## ğŸ” CVEåˆ†ç±»ç»Ÿè®¡\n\næ•°æ®ç»Ÿè®¡ä¸­ï¼Œæ•¬è¯·æœŸå¾…...\n"
    
    # æ·»åŠ POC/EXPç»Ÿè®¡
    if poc_exp_stats:
        md_content += "\n## ğŸ› ï¸ POC/EXPç»Ÿè®¡\n\n| ç±»å‹ | æ•°é‡ | å æ¯” |\n|:---|:---|:---|\n"
        total_poc_exp = sum(poc_exp_stats.values())
        for p_type, count in poc_exp_stats.items():
            percentage = (count / total_poc_exp * 100) if total_poc_exp > 0 else 0
            md_content += f"| {p_type} | {count:,} | {percentage:.1f}% |\n"
    else:
        md_content += "\n## ğŸ› ï¸ POC/EXPç»Ÿè®¡\n\næ•°æ®ç»Ÿè®¡ä¸­ï¼Œæ•¬è¯·æœŸå¾…...\n"
    
    # æ·»åŠ å‚å•†ç»Ÿè®¡
    if vendor_stats:
        md_content += "\n## ğŸ¢ å‚å•†æ¼æ´ç»Ÿè®¡Top 10\n\n| å‚å•† | æ¼æ´æ•°é‡ |\n|:---|:---|\n"
        for vendor, count in vendor_stats.items():
            md_content += f"| {vendor} | {count:,} |\n"
    else:
        md_content += "\n## ğŸ¢ å‚å•†æ¼æ´ç»Ÿè®¡Top 10\n\næ•°æ®ç»Ÿè®¡ä¸­ï¼Œæ•¬è¯·æœŸå¾…...\n"
    
    # æ·»åŠ æ•°æ®è·å–æ¶æ„è¯´æ˜
    md_content += """\n## ğŸ’¡ æ•°æ®è·å–æ¶æ„

ç³»ç»Ÿé‡‡ç”¨é«˜æ•ˆçš„æ•°æ®è·å–å’Œç¼“å­˜æœºåˆ¶ï¼š

### æ ¸å¿ƒç‰¹ç‚¹
- **è‡ªåŠ¨åŒ–æ›´æ–°**: é€šè¿‡GitHub Actionså®šæ—¶è·å–å¹¶å¤„ç†æ•°æ®
- **é™æ€æ–‡ä»¶ç¼“å­˜**: æ•°æ®å­˜å‚¨ä¸ºJSONæ–‡ä»¶ï¼Œæé«˜è®¿é—®é€Ÿåº¦
- **æ— Tokenä¾èµ–**: å‰ç«¯ç›´æ¥åŠ è½½é™æ€æ•°æ®ï¼Œæ— éœ€APIå¯†é’¥
- **å®æ—¶ç»Ÿè®¡**: è‡ªåŠ¨ç”Ÿæˆå¤šç»´åº¦ç»Ÿè®¡åˆ†æ

### æ€§èƒ½ä¼˜åŠ¿
- é¡µé¢åŠ è½½é€Ÿåº¦æå‡çº¦80%
- é¿å…GitHub APIé€Ÿç‡é™åˆ¶é—®é¢˜
- æé«˜ç³»ç»Ÿç¨³å®šæ€§å’Œå¯é æ€§
- æ”¯æŒç¦»çº¿è®¿é—®å·²ç¼“å­˜æ•°æ®
"""
    
    # æ·»åŠ è¯´æ˜
    md_content += "\n## â„¹ï¸ æ•°æ®è¯´æ˜\n- æ•°æ®æ¥æºäºæ¯æ—¥ä»GitHubæ”¶é›†çš„CVEä¿¡æ¯\n- ç»Ÿè®¡åŸºäºå·²æ”¶é›†çš„æ•°æ®ï¼Œå¯èƒ½å­˜åœ¨å»¶è¿Ÿ\n- åˆ†ç±»ç»Ÿè®¡åŸºäºå…³é”®è¯åŒ¹é…ï¼Œä»…ä¾›å‚è€ƒ\n- ç»Ÿè®¡æ•°æ®æ¯æ—¥è‡ªåŠ¨æ›´æ–°"
    
    # ä¿å­˜Markdownæ–‡ä»¶
    try:
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        output_dir = os.path.dirname(output_md_path)
        if output_dir and not os.path.exists(output_dir):
            print(f"ğŸ“ åˆ›å»ºè¾“å‡ºç›®å½•: {output_dir}")
            os.makedirs(output_dir, exist_ok=True)
        
        with open(output_md_path, 'w', encoding='utf-8') as f:
            f.write(md_content)
        print(f"âœ… Wikiç»Ÿè®¡Markdownå·²ç”Ÿæˆ: {output_md_path}")
        return True
    except Exception as e:
        print(f"âŒ ç”ŸæˆMarkdownå¤±è´¥: {e}")
        return False

def main():
    # è·å–è„šæœ¬æ‰€åœ¨ç›®å½•çš„ç»å¯¹è·¯å¾„
    script_dir = os.path.dirname(os.path.abspath(__file__))
    project_root = os.path.dirname(script_dir)
    
    # è®¾ç½®é»˜è®¤è·¯å¾„ä¸ºç»å¯¹è·¯å¾„ - ä½¿ç”¨å°å†™çš„dataç›®å½•
    default_summary = os.path.join(project_root, 'docs', 'data', 'daily', 'daily_summary.json')
    default_daily_dir = os.path.join(project_root, 'docs', 'data', 'daily')
    default_output_json = os.path.join(project_root, 'docs', 'data', 'statistics', 'wiki_stats.json')
    default_output_md = os.path.join(project_root, 'wiki_content', 'ç»Ÿè®¡æ•°æ®.md')
    
    parser = argparse.ArgumentParser(description='Wikiç»Ÿè®¡æ•°æ®ç”Ÿæˆå™¨')
    parser.add_argument('--summary', '-s',
                       default=default_summary,
                       help=f'æ¯æ—¥æ±‡æ€»æ–‡ä»¶è·¯å¾„ (é»˜è®¤: {default_summary})')
    parser.add_argument('--daily-dir', '-d',
                       default=default_daily_dir,
                       help=f'æ¯æ—¥æ•°æ®ç›®å½• (é»˜è®¤: {default_daily_dir})')
    parser.add_argument('--output-json', '-j',
                       default=default_output_json,
                       help=f'è¾“å‡ºç»Ÿè®¡JSONæ–‡ä»¶è·¯å¾„ (é»˜è®¤: {default_output_json})')
    parser.add_argument('--output-md', '-m',
                       default=default_output_md,
                       help=f'è¾“å‡ºWiki Markdownæ–‡ä»¶è·¯å¾„ (é»˜è®¤: {default_output_md})')
    parser.add_argument('--days', '-n',
                       type=int, default=30,
                       help='ç»Ÿè®¡å¤©æ•° (é»˜è®¤: 30)')
    
    args = parser.parse_args()
    
    print("ğŸš€ GitHub CVE Monitor - Wikiç»Ÿè®¡æ•°æ®ç”Ÿæˆå™¨")
    print("=" * 60)
    
    # åŠ è½½æ±‡æ€»æ•°æ®
    print(f"ğŸ“Š åŠ è½½æ±‡æ€»æ•°æ®: {args.summary}")
    summary = load_daily_summary(args.summary)
    if not summary:
        print("âŒ æ— æ³•åŠ è½½æ±‡æ€»æ•°æ®ï¼Œè„šæœ¬é€€å‡º")
        return 1
    
    # åŠ è½½æ¯æ—¥æ•°æ®
    print(f"ğŸ“… åŠ è½½æœ€è¿‘{args.days}å¤©çš„æ¯æ—¥æ•°æ®...")
    daily_files = load_daily_files(args.daily_dir, args.days)
    print(f"âœ… æˆåŠŸåŠ è½½ {len(daily_files)} ä¸ªæ¯æ—¥æ•°æ®æ–‡ä»¶")
    
    # ç”Ÿæˆç»Ÿè®¡æ•°æ®
    print("ğŸ“ˆ ç”Ÿæˆç»Ÿè®¡æ•°æ®...")
    stats = generate_stats_file(summary, daily_files, args.output_json)
    if not stats:
        print("âŒ ç»Ÿè®¡æ•°æ®ç”Ÿæˆå¤±è´¥")
        return 1
    
    # ç”ŸæˆWiki Markdown
    print("ğŸ“ ç”ŸæˆWikiç»Ÿè®¡Markdown...")
    if generate_wiki_md(stats, args.output_md):
        print("\nâœ… æ‰€æœ‰ç»Ÿè®¡æ•°æ®å·²æˆåŠŸç”Ÿæˆï¼")
        return 0
    else:
        print("\nâŒ Wiki Markdownç”Ÿæˆå¤±è´¥")
        return 1

if __name__ == '__main__':
    exit(main())