#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¿«é€Ÿä¿®å¤å¢é•¿ç‡è®¡ç®—å’Œç¼ºå¤±æ–‡ä»¶é—®é¢˜

è¿™ä¸ªè„šæœ¬å°†:
1. åˆ†æç°æœ‰çš„dailyç›®å½•
2. ç”Ÿæˆç¼ºå¤±æ—¥æœŸçš„JSONæ–‡ä»¶ 
3. é‡æ–°è®¡ç®—æ­£ç¡®çš„å¢é•¿ç‡
4. æ›´æ–°daily_summary.json
"""

import os
import json
import re
from datetime import datetime, timedelta, date
from collections import defaultdict
import argparse
from pathlib import Path

def analyze_existing_files(daily_dir):
    """åˆ†æç°æœ‰çš„æ¯æ—¥æ–‡ä»¶"""
    existing_files = []
    daily_counts = {}
    
    if not os.path.exists(daily_dir):
        print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {daily_dir}")
        return existing_files, daily_counts
    
    print(f"ğŸ” åˆ†æç›®å½•: {daily_dir}")
    
    for filename in os.listdir(daily_dir):
        if filename.endswith('.json') and filename != 'daily_summary.json':
            # æå–æ—¥æœŸ
            date_match = re.match(r'(\d{4}-\d{2}-\d{2})\.json', filename)
            if date_match:
                date_str = date_match.group(1)
                filepath = os.path.join(daily_dir, filename)
                
                try:
                    with open(filepath, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                    
                    count = data.get('count', 0)
                    existing_files.append({
                        'file': filename,
                        'date': date_str,
                        'count': count,
                        'path': filepath
                    })
                    daily_counts[date_str] = count
                    
                except Exception as e:
                    print(f"âš ï¸ è¯»å–æ–‡ä»¶å¤±è´¥ {filename}: {e}")
    
    # æŒ‰æ—¥æœŸæ’åº
    existing_files.sort(key=lambda x: x['date'])
    
    print(f"âœ… åˆ†æå®Œæˆï¼Œæ‰¾åˆ° {len(existing_files)} ä¸ªæœ‰æ•ˆæ–‡ä»¶")
    return existing_files, daily_counts

def find_missing_dates(daily_counts):
    """æ‰¾å‡ºç¼ºå¤±çš„æ—¥æœŸ"""
    if not daily_counts:
        return []
    
    dates = list(daily_counts.keys())
    start_date = datetime.strptime(min(dates), '%Y-%m-%d').date()
    end_date = datetime.strptime(max(dates), '%Y-%m-%d').date()
    
    missing_dates = []
    current_date = start_date
    
    while current_date <= end_date:
        date_str = current_date.isoformat()
        if date_str not in daily_counts:
            missing_dates.append(date_str)
        current_date += timedelta(days=1)
    
    print(f"ğŸ” å‘ç° {len(missing_dates)} ä¸ªç¼ºå¤±æ—¥æœŸ")
    return missing_dates

def create_empty_json_files(missing_dates, daily_dir):
    """ä¸ºç¼ºå¤±çš„æ—¥æœŸåˆ›å»ºç©ºçš„JSONæ–‡ä»¶"""
    created_files = []
    
    for date_str in missing_dates:
        filename = f"{date_str}.json"
        filepath = os.path.join(daily_dir, filename)
        
        json_data = {
            'date': date_str,
            'count': 0,
            'cves': [],
            'generated_at': datetime.now().isoformat(),
            'metadata': {
                'total_cves': 0,
                'date_range': date_str,
                'source': 'auto_generated_empty',
                'script_version': '2.0'
            }
        }
        
        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(json_data, f, ensure_ascii=False, indent=2)
            
            created_files.append({
                'file': filename,
                'date': date_str,
                'count': 0,
                'path': filepath
            })
            
        except Exception as e:
            print(f"âŒ åˆ›å»ºæ–‡ä»¶å¤±è´¥ {filename}: {e}")
    
    print(f"âœ… åˆ›å»ºäº† {len(created_files)} ä¸ªç©ºæ–‡ä»¶")
    return created_files

def calculate_growth_stats(all_files):
    """è®¡ç®—æ­£ç¡®çš„å¢é•¿ç»Ÿè®¡ä¿¡æ¯"""
    if len(all_files) < 2:
        return []
    
    # æŒ‰æ—¥æœŸæ’åº
    sorted_files = sorted(all_files, key=lambda x: x['date'])
    growth_stats = []
    cumulative_total = 0
    
    print(f"ğŸ“ˆ è®¡ç®—å¢é•¿ç‡...")
    
    for i, file_data in enumerate(sorted_files):
        cumulative_total += file_data['count']
        
        # è®¡ç®—å¢é•¿ç‡ - ä¿®å¤åçš„é€»è¾‘
        if i > 0:
            prev_count = sorted_files[i-1]['count']
            if prev_count > 0:
                growth_rate = ((file_data['count'] - prev_count) / prev_count) * 100
            else:
                growth_rate = 0 if file_data['count'] == 0 else float('inf')
                # å¤„ç†æ— ç©·å¤§çš„æƒ…å†µ
                if growth_rate == float('inf'):
                    growth_rate = 100.0
        else:
            growth_rate = 0
        
        growth_stats.append({
            'date': file_data['date'],
            'daily_count': file_data['count'],
            'cumulative_total': cumulative_total,
            'growth_rate': round(growth_rate, 2) if growth_rate != float('inf') else 100.0
        })
    
    print(f"âœ… è®¡ç®—å®Œæˆï¼Œ{len(growth_stats)} ä¸ªæ•°æ®ç‚¹")
    return growth_stats

def update_summary_file(all_files, daily_dir):
    """æ›´æ–°daily_summary.jsonæ–‡ä»¶"""
    growth_stats = calculate_growth_stats(all_files)
    
    summary = {
        'generated_at': datetime.now().isoformat(),
        'script_version': '2.0_hotfix',
        'total_files': len(all_files),
        'total_cves': sum(f['count'] for f in all_files),
        'date_range': {
            'start': min(f['date'] for f in all_files) if all_files else None,
            'end': max(f['date'] for f in all_files) if all_files else None
        },
        'statistics': {
            'avg_daily_cves': round(sum(f['count'] for f in all_files) / len(all_files), 2) if all_files else 0,
            'max_daily_cves': max(f['count'] for f in all_files) if all_files else 0,
            'min_daily_cves': min(f['count'] for f in all_files) if all_files else 0,
            'empty_days': len([f for f in all_files if f['count'] == 0]),
            'active_days': len([f for f in all_files if f['count'] > 0])
        },
        'growth_analysis': growth_stats,
        'recent_7_days': growth_stats[-7:] if len(growth_stats) >= 7 else growth_stats,
        'recent_30_days': growth_stats[-30:] if len(growth_stats) >= 30 else growth_stats,
        'daily_stats': all_files,
        'hotfix_notes': 'ä¿®å¤äº†å¢é•¿ç‡è®¡ç®—é”™è¯¯ï¼Œæ·»åŠ äº†ç¼ºå¤±æ—¥æœŸçš„ç©ºæ–‡ä»¶'
    }
    
    # ä¿å­˜æ±‡æ€»æ–‡ä»¶
    summary_path = os.path.join(daily_dir, 'daily_summary.json')
    try:
        with open(summary_path, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        print(f"ğŸ“Š æ±‡æ€»æ–‡ä»¶å·²æ›´æ–°: {summary_path}")
        return summary
    except Exception as e:
        print(f"âŒ æ›´æ–°æ±‡æ€»æ–‡ä»¶å¤±è´¥: {e}")
        return None

def main():
    parser = argparse.ArgumentParser(description='å¿«é€Ÿä¿®å¤æ¯æ—¥æ•°æ®çš„å¢é•¿ç‡è®¡ç®—å’Œç¼ºå¤±æ–‡ä»¶é—®é¢˜')
    parser.add_argument('--daily-dir', '-d',
                       default='../docs/Data/daily',
                       help='æ¯æ—¥æ•°æ®ç›®å½• (é»˜è®¤: ../docs/Data/daily)')
    parser.add_argument('--create-missing', '-c',
                       action='store_true',
                       help='åˆ›å»ºç¼ºå¤±æ—¥æœŸçš„ç©ºJSONæ–‡ä»¶')
    parser.add_argument('--verbose', '-v', 
                       action='store_true',
                       help='æ˜¾ç¤ºè¯¦ç»†è¾“å‡º')
    
    args = parser.parse_args()
    
    print("ğŸ”§ GitHub CVE Monitor - å¿«é€Ÿä¿®å¤å·¥å…·")
    print("=" * 50)
    
    # åˆ†æç°æœ‰æ–‡ä»¶
    existing_files, daily_counts = analyze_existing_files(args.daily_dir)
    
    if not existing_files:
        print("âŒ æœªæ‰¾åˆ°ä»»ä½•æœ‰æ•ˆçš„æ•°æ®æ–‡ä»¶")
        return 1
    
    all_files = existing_files.copy()
    
    # å¤„ç†ç¼ºå¤±æ—¥æœŸ
    if args.create_missing:
        missing_dates = find_missing_dates(daily_counts)
        if missing_dates:
            created_files = create_empty_json_files(missing_dates, args.daily_dir)
            all_files.extend(created_files)
            print(f"âœ… æˆåŠŸåˆ›å»º {len(created_files)} ä¸ªç¼ºå¤±æ–‡ä»¶")
        else:
            print("â„¹ï¸ æ²¡æœ‰å‘ç°ç¼ºå¤±çš„æ—¥æœŸ")
    
    # æ›´æ–°æ±‡æ€»æ–‡ä»¶
    print("\nğŸ“Š æ›´æ–°æ±‡æ€»ç»Ÿè®¡...")
    summary = update_summary_file(all_files, args.daily_dir)
    
    if summary:
        print("\n" + "=" * 50)
        print("âœ… ä¿®å¤å®Œæˆï¼")
        print(f"ğŸ“ æ€»æ–‡ä»¶æ•°: {summary['total_files']}")
        print(f"ğŸ“Š æ€»CVEæ•°é‡: {summary['total_cves']}")
        print(f"ğŸ“… æ—¥æœŸèŒƒå›´: {summary['date_range']['start']} åˆ° {summary['date_range']['end']}")
        print(f"ğŸ“ˆ å¹³å‡æ¯æ—¥CVE: {summary['statistics']['avg_daily_cves']}")
        print(f"ğŸ¯ æ´»è·ƒå¤©æ•°: {summary['statistics']['active_days']}")
        print(f"ğŸ’¤ ç©ºç™½å¤©æ•°: {summary['statistics']['empty_days']}")
        
        if args.verbose:
            print("\nğŸ“ˆ æœ€è¿‘7å¤©å¢é•¿è¶‹åŠ¿:")
            recent_growth = summary['recent_7_days']
            for day in recent_growth[-7:]:
                growth_indicator = "ğŸ“ˆ" if day['growth_rate'] > 0 else "ğŸ“‰" if day['growth_rate'] < 0 else "â¡ï¸"
                print(f"   - {day['date']}: {day['daily_count']} ä¸ªCVE (å¢é•¿ç‡: {day['growth_rate']:+.1f}%) {growth_indicator}")
    
    return 0

if __name__ == '__main__':
    exit(main())